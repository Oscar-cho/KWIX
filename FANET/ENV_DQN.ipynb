{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class FANET:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.map_point = {\n",
    "            'hazard 1': {'start': (10, 0, 0), 'end': (40, 20, 50), 'reward' : -10},\n",
    "            'hazard 2': {'start': (60, 60, 0), 'end': (95, 95, 80), 'reward' : -10},\n",
    "            'base station' : {'start': (0, 0, 0), 'end': (1, 1, 1), 'reward' : 0},\n",
    "            'terminal zone' : {'start' : (90, 90, 90), 'end': (100, 100, 100), 'reward' : 1}\n",
    "        }\n",
    "        \n",
    "        self.grid_map = self.map()\n",
    "        self.signal_map = self.signal_grid()\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def set_start_zone(self):\n",
    "        x = np.random.randint(3, self.size[0] - 30)\n",
    "        y = np.random.randint(3, self.size[1] - 30)\n",
    "        z = 0\n",
    "    \n",
    "        if self.grid_map[x, y, z] not in [-10, 1]:\n",
    "            return (x, y, z)\n",
    "        else:\n",
    "            return self.set_start_zone()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.set_start_zone()\n",
    "   \n",
    "    def map(self):  # reward mapping\n",
    "        grid = -1 * np.ones(self.size, dtype=np.float32)\n",
    "        \n",
    "        for zone in self.map_point.values():\n",
    "            start = zone['start']\n",
    "            end = zone['end']\n",
    "            reward = zone['reward']\n",
    "            grid[start[0]:end[0]+1, start[1]:end[1]+1, start[2]:end[2]+1] = reward\n",
    "        \n",
    "        return grid\n",
    "    \n",
    "    def distance_from_BS(self, x, y, z):    # 현재 state에서의 거리율에 따른 FSPL을 계산 용도 -> 기지국에서 지정 Cell 까지의 거리 계산 및 그 이후의 FSPL을 계산하는 용도로 변환해야함\n",
    "        bx1, by1, bz1 = self.map_point['base station']['start']\n",
    "        bx2, by2, bz2 = self.map_point['base station']['end']\n",
    "        \n",
    "        '''\n",
    "        dx = max(abs(x - bx1), abs(x - bx2))\n",
    "        dy = max(abs(y - by1), abs(y - by2))\n",
    "        dz = max(abs(z - bz1), abs(z - bz2))\n",
    "        '''\n",
    "\n",
    "        # 기지국의 중심좌표를 이용하여 계산\n",
    "        dx = abs(x - (bx1 + bx2) / 2)\n",
    "        dy = abs(y - (by1 + by2) / 2)\n",
    "        dz = abs(z - (bz1 + bz2) / 2)\n",
    "\n",
    "        distance = np.sqrt(dx ** 2 + dy ** 2 + dz ** 2)\n",
    "        \n",
    "        return distance\n",
    "        \n",
    "    def FSPL(self, x, y, z):\n",
    "        frequency = 2400000000  # 2.4 GHz / 5 GHz\n",
    "        distance = self.distance_from_BS(x,y,z)\n",
    "\n",
    "        free_space_path_loss_db = 20 * np.log10(distance) + 20 * np.log10(frequency) - 147.55  \n",
    "        \n",
    "        return free_space_path_loss_db\n",
    "    \n",
    "    def signal_grid(self):\n",
    "        signal_grid = np.zeros(self.size, dtype=np.float32)\n",
    "        '''\n",
    "        기지국으로 부터 떨어진 거리 Loss를 mapping.\n",
    "        '''  \n",
    "        print('Calculating Path Loss...')\n",
    "        # 거리에 따른 통신감쇄율(dB) Mapping\n",
    "        for x in tqdm(range(self.size[0])):\n",
    "            for y in range(self.size[1]):\n",
    "                for z in range(self.size[2]):\n",
    "                    signal_grid[x, y, z] = self.FSPL(x, y, z)\n",
    "             \n",
    "        #start = self.map_point['base station']['start']\n",
    "        #end = self.map_point['base station']['end']       \n",
    "        #signal_grid[start[0]:end[0]+1, start[1]:end[1]+1, start[2]:end[2]+1] = 0   # 기지국 mapping\n",
    "        \n",
    "        print('Done!')\n",
    "        \n",
    "        return signal_grid\n",
    "                    \n",
    "    def reward(self, x, y, z):\n",
    "        '''Sensor 범위 (+-2) 만큼으로 Conflict 됐을 때의 reward를 계산'''\n",
    "        if self.grid_map[x+2, y+2, z] == -10 or self.grid_map[x-2, y-2, z]:    \n",
    "            reward = -10\n",
    "        elif self.grid_map[x+2, y+2, z] == -10 and self.grid_map[x-2, y-2, z]:\n",
    "            reward = -20\n",
    "        elif self.grid_map[x, y, z] == 1:   # Terminal State\n",
    "            reward = 1\n",
    "        else:                               # Default Reward\n",
    "            reward = -1\n",
    "        \n",
    "        '''\n",
    "        if self.grid_map[x, y, z] == -10:\n",
    "            reward = -10\n",
    "        elif self.grid_map[x, y, z] == 1:   # Terminal State\n",
    "            reward = 1\n",
    "        else:                               # Default Reward\n",
    "            reward = -1\n",
    "        '''\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def termianl_reward(self, step_count):\n",
    "        if step_count <= 100 * 100:\n",
    "            return 30\n",
    "        elif step_count <= 100 * 100* 100:\n",
    "            return -10\n",
    "        else:\n",
    "            return -50\n",
    "    \n",
    "    def step(self, action):\n",
    "        x, y, z = self.state\n",
    "        \n",
    "        if action == 0 and x < self.size[0] - 3:\n",
    "            x += 1\n",
    "        elif action == 1 and x > 3:\n",
    "            x -= 1\n",
    "        elif action == 2 and y < self.size[1] - 3:\n",
    "            y += 1\n",
    "        elif action == 3 and y > 3:\n",
    "            y -= 1\n",
    "        elif action == 4 and z < self.size[2] - 1:\n",
    "            z += 1\n",
    "        elif action == 5 and z > 1:\n",
    "            z -= 1\n",
    "        \n",
    "        reward = self.reward(x, y, z)    \n",
    "        self.state = (x, y, z)\n",
    "        \n",
    "        return self.state, reward, self.grid_map[x, y, z] == self.map_point['terminal zone']['reward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_state, action_size):\n",
    "        super(DQN, self).__init__() \n",
    "        self.layer1 = nn.Linear(input_state, 128)   # input = env.size (100+100+100)\n",
    "        self.layer2 = nn.Linear(128, action_size)    # output = action (6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        return self.layer2(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, gamma, learning_rate, batch_size, replay_buffer_size, target_update_interval):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.target_update_interval = target_update_interval\n",
    "        \n",
    "        self.state_size = env.size[0] + env.size[1] + env.size[2]\n",
    "        self.action_size = 6\n",
    "        \n",
    "        self.policy_model_path = 'DQN_policy_network.pth'\n",
    "        self.policy_network = DQN(self.state_size, self.action_size).cuda()\n",
    "        \n",
    "        if os.path.exists(self.policy_model_path):\n",
    "            self.policy_network.load_state_dict(torch.load(self.policy_model_path))\n",
    "            self.policy_network.eval()\n",
    "            print('Policy Model Loaded Successfully.')\n",
    "        else:\n",
    "            print('No Model Founded.')\n",
    "            \n",
    "        self.target_model = DQN(self.state_size, self.action_size).cuda()\n",
    "        self.target_model.load_state_dict(self.policy_network.state_dict())\n",
    "        self.target_model.eval()\n",
    "        \n",
    "\n",
    "        \n",
    "        self.Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "        self.replay_buffer = deque(maxlen=self.replay_buffer_size)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=self.learning_rate)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        \n",
    "        self.epsilon_start = 1.0\n",
    "        self.epsilon_end = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon = self.epsilon_start\n",
    "        \n",
    "    def encode_state(self, state):\n",
    "        x, y, z = state\n",
    "        x_onehot, y_onehot, z_onehot = np.zeros(self.env.size[0]), np.zeros(self.env.size[1]), np.zeros(self.env.size[2])\n",
    "        \n",
    "        x_onehot[x] = 1\n",
    "        y_onehot[y] = 1\n",
    "        z_onehot[z] = 1\n",
    "        \n",
    "        return np.concatenate((x_onehot, y_onehot, z_onehot))\n",
    "    \n",
    "    def decode_state(self, encoded_state):\n",
    "        x, y, z = self.env.size\n",
    "        \n",
    "        x_encoded = encoded_state[:x]\n",
    "        y_encoded = encoded_state[x : x+y]\n",
    "        z_encoded = encoded_state[x+y : x+y+z]\n",
    "        \n",
    "        x = np.argmax(x_encoded)\n",
    "        y = np.argmax(y_encoded)\n",
    "        z = np.argmax(z_encoded)\n",
    "        \n",
    "        return (x, y, z)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.randint(self.action_size)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.policy_network(torch.tensor(state, dtype = torch.float32).cuda())\n",
    "                action = torch.argmax(q_values).item()\n",
    "        return action\n",
    "    \n",
    "    def deepQ(self, num_episodes):\n",
    "        self.num_episodes = num_episodes\n",
    "        \n",
    "        # 성능 지표 변수 선언 부분\n",
    "        step_count = [0] * self.num_episodes\n",
    "        conflict_count = [0] * self.num_episodes\n",
    "        path = [0] * self.num_episodes\n",
    "        time_count = [0] * self.num_episodes\n",
    "        loss_count = [0] * self.num_episodes\n",
    "        # 성능 지표 변수 선언 부분\n",
    "        print('Learning Start! ...')\n",
    "        \n",
    "        for episode in tqdm(range(self.num_episodes)):\n",
    "            start_time = time.time()\n",
    "            self.env.reset()\n",
    "            state = self.env.state\n",
    "            state = self.encode_state(state)\n",
    "            \n",
    "            # 성능 지표 변수 선언 부분\n",
    "            # 처음 위치를 decode로 사용해야 후에 path 경로를 출력할 때 TypeError를 방지함\n",
    "            decoded_state = self.decode_state(state)    \n",
    "            path_epi = [decoded_state]\n",
    "            done = False\n",
    "            step = 0\n",
    "            conflict = 0\n",
    "            # 성능 지표 변수 선언 부분\n",
    "            \n",
    "            while not done:\n",
    "                action = self.get_action(state)\n",
    "                        \n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                \n",
    "                step += 1\n",
    "                \n",
    "                if step % 10000 == 0:\n",
    "                    print(f'현재 Episode {episode+1}의 진행 수는 {step}입니다.')\n",
    "                    \n",
    "                if reward == -10:\n",
    "                    conflict += 1\n",
    "                elif reward == -20:\n",
    "                    conflict += 2\n",
    "                \n",
    "                next_state = self.encode_state(next_state)\n",
    "                \n",
    "                decode_next_state = self.decode_state(next_state)\n",
    "                path_epi.append(decode_next_state)\n",
    "                \n",
    "                transition = self.Transition(state, action, next_state, reward, done)\n",
    "                self.replay_buffer.append(transition)\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "                if len(self.replay_buffer) >= self.batch_size:\n",
    "                    batch = np.random.choice(len(self.replay_buffer), self.batch_size, replace=False)\n",
    "                    batch = [self.replay_buffer[i] for i in batch]\n",
    "                    batch_state, batch_action, batch_next_state, batch_reward, batch_done = zip(*batch)\n",
    "                    \n",
    "                    batch_state = torch.tensor(np.array(batch_state), dtype=torch.float32).cuda()   \n",
    "                    batch_action = torch.tensor(np.array(batch_action), dtype=torch.long).unsqueeze(1).cuda()\n",
    "                    batch_next_state = torch.tensor(np.array(batch_next_state), dtype=torch.float32).cuda()\n",
    "                    batch_reward = torch.tensor(np.array(batch_reward), dtype=torch.float32).unsqueeze(1).cuda()\n",
    "                    batch_done = torch.tensor(np.array(batch_done), dtype=torch.float32).unsqueeze(1).cuda()\n",
    "                    \n",
    "                    current_q_values = self.policy_network(batch_state).gather(1, batch_action)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        next_q_values = self.target_model(batch_next_state).max(1)[0].unsqueeze(1)\n",
    "                    target_q_values = batch_reward + (1 - batch_done) * self.gamma * next_q_values\n",
    "                    \n",
    "                    loss = self.loss_fn(current_q_values, target_q_values)\n",
    "                    \n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            end_time = time.time()\n",
    "            step_count[episode] = step\n",
    "            conflict_count[episode] = conflict\n",
    "            path[episode] = path_epi\n",
    "            time_count[episode] = end_time - start_time\n",
    "            loss_count[episode] = loss\n",
    "            \n",
    "            self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "            \n",
    "            if episode % self.target_update_interval == 0:\n",
    "                self.target_model.load_state_dict(self.policy_network.state_dict())\n",
    "                \n",
    "            \n",
    "        print('Done!')\n",
    "        \n",
    "        print('Model Save...')\n",
    "        torch.save(self.policy_network.state_dict(), self.policy_model_path)\n",
    "        print('Model Save Done!')\n",
    "        \n",
    "        return step_count, conflict_count, path, time_count, loss_count\n",
    "        \n",
    "env = FANET((100, 100, 100))\n",
    "agent = DQNAgent(env, gamma=0.999, learning_rate=0.0001, batch_size=32, replay_buffer_size=500000, target_update_interval=100)\n",
    "\n",
    "step_count, conflict_count, path, time_count, loss_count = agent.deepQ(num_episodes=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_3d(env, path):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    hazard_indices = np.where(env.grid_map == -10)\n",
    "    terminal_indices = np.where(env.grid_map == 1)\n",
    "\n",
    "    ax.scatter(*hazard_indices, c='r', marker='s', label='Hazard Zone')\n",
    "    ax.scatter(*terminal_indices, c='g', marker='s', label='Terminal Zone')\n",
    "\n",
    "    x, y, z = np.array(path).T\n",
    "    ax.plot(x, y, z, '>', label='Path')\n",
    "\n",
    "    ax.set_xlim(-10, env.size[0]+10)\n",
    "    ax.set_ylim(-10, env.size[1]+10)\n",
    "    ax.set_zlim(0, env.size[2]+10)\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def visualize_2d(env, path):\n",
    "    hazard_indices = np.where(env.grid_map == -10)\n",
    "    terminal_indices = np.where(env.grid_map == 1)\n",
    "\n",
    "    plt.scatter(*hazard_indices[:2], c='r', marker='s', label='Hazard Zone')\n",
    "    plt.scatter(*terminal_indices[:2], c='g', marker='s', label='Terminal Zone')\n",
    "\n",
    "    x, y, _ = np.array(path).T\n",
    "    plt.plot(x, y, '-o', label='Path')\n",
    "\n",
    "    plt.xlim(-10, env.size[0]+10)\n",
    "    plt.ylim(-10, env.size[1]+10)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "# Step Count\n",
    "plt.plot(step_count)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Steps')\n",
    "plt.title('Q-Learning')\n",
    "plt.show() \n",
    "        \n",
    "# Conflict Count\n",
    "plt.plot(conflict_count, color='orange', label = 'Num of Conflict') \n",
    "plt.legend()\n",
    "plt.show()    \n",
    "\n",
    "# Path / 마지막 에피소드만 출력!\n",
    "visualize_3d(env, path[-1])\n",
    "visualize_2d(env, path[-1])\n",
    "\n",
    "# Time\n",
    "plt.plot(time_count)\n",
    "plt.title(\"Time Duration\")\n",
    "plt.show()\n",
    "\n",
    "# Loss -> 오류남 -> 오류 수정 완료\n",
    "# Torch를 바로 plot 하면 오류, np로 바꿔줌\n",
    "plt.plot([loss.item() for loss in loss_count])\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
